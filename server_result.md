# 在服务器上 GridSearch 超参数的记录

此文件记录了在 bash 脚本 param_select.sh 指令下搜索最佳参数的结果。

## 搜索 word_embedding 和 slot_embedding 的维度

剩余未调节参数：使用优化算法 adagrad、batch 大小 32、训练轮数 800、Encoder 层数 1、LSTM 隐层大小 64。

| w/s 维度          | 128/64   | 128/32   | 128/16   |  128/8   | 64/64     | 64/32      | 64/16    | 64/8   |
| :-----------      | :----:   | :---:    | :---:    |  :---:   | :---:     | :---:      | :----:   | :---:  |
|   **SF**   F_1    | 0.917340 | 0.932424 | 0.929261 | 0.932256 | 0.920261  | 0.933458   | 0.932752 | 0.934452 |
|   **ID** Accuracy | 0.950000 | 0.928000 | 0.928000 | 0.930000 | 0.914000  | 0.910000   | 0.926000 | 0.908000 |

| w/s 维度           | 32/64    | 32/32    |  32/16  | 32/8    | 
| :-----------       |:----:    |:----:    |:----:   | :----:   |
|   **SF**   F_1    | 0.922693 | 0.915964 | 0.920299 | 0.926204  |
|   **ID** Accuracy | 0.910000 | 0.926000 | 0.910000 | 0.930000  |

经过权衡计算量、Slot Filling 的 F1 值、Intent Detection 的精确度，**选择参数 w/s 维度 = 64/16**。

## 搜索 batch 大小和 LSTM 隐层大小
剩余未调节参数：使用优化算法 adagrad、训练轮数 **2000**(保证在 batch 很小时也能完整读过 3+ 次数据)、Encoder 层数 1。

| b/h 大小          | 8/64   | 8/128   | 8/256   |  16/64   | 16/128     | 16/256      | 32/64    | 32/128   |
| :-----------      | :----:   | :---:    | :---:    |  :---:   | :---:     | :---:      | :----:   | :---:  |
|   **SF**   F_1    | 0.915603 | 0.926466 | 0.925096 | 0.928660 | 0.932214  | 0.919440  | 0.940150 | 0.941323 |
|   **ID** Accuracy | 0.934000 | 0.946000 | 0.942000 | 0.928000 | 0.918000  | 0.922000   | 0.946000 | 0.936000 |
| 收敛轮数      | 1560    |   1590  |  1680   |  1980    |   960      |    1260  |  1320    |   480    |

|  b/h 大小           | 32/256    | 64/64    |  64/128  | 64/256    | 
| :-----------       |:----:    |:----:    |:----:   | :----:   |
|   **SF**   F_1    | 0.911811 | 0.945274 | 0.952912 | 0.927970  |
|   **ID** Accuracy | 0.916000 | 0.936000 | 0.942000 | 0.890000  |
| 收敛轮数           | 1050|  690   | 540      |  870     |

通过诸方面比较(注意 **SF** 的效果应该优先考虑)，应该**选择 batch 大小为 64, 隐层 hidden 维度大小为 128**。

## 搜索 learning_rate 大小和 optimizer 算法
注意上述选择的最优参数 64/128 在 540 轮时就收敛了，但由于学习率过小可能会导致收敛缓慢，因此这次调整训练轮数为 
**1200**。其余参数可由上述实验确定，其中 LSTM 隐层现在仍然默认为 1。

| 算法/lr 大小      | adam/0.01 | adagrad/0.01 | adagrad/0.05 | adagrad 0.1 | 
| :-----------      | :----:   |   :---:      | :---:        | :---:      | 
|   **SF**   F_1    | 0.936    | 0.895879     | 0.944358     | 0.924177  | 
|   **ID** Accuracy | 0.948    | 0.842000     | 0.906000      | 0.930000   | 
| 收敛轮数           |  960     |   ----       |    1020      |  800    |  

由于有些算法配合学习率其效果不堪入目，仅展示略有成效的组合。特别的，SGD 优化算法在这个模型的训练上是失败的，收敛缓慢是其次，
其在 slot filling 的 F1 值还不到 0.7。不难发现**使用优化算法 adam 和学习率 0.01 是最好的**。

## 搜索 LSTM 网络层数 hidden
使用以上实验中确定的最佳参数并设置训练轮数 1200。

|  LSTM 层数         | 1         | 2       |  3      |
| :-----------      | :----:    |:----:    |:----:   |
|   **SF**   F_1    | 0.948084 | 0.930290 | 0.909709 |
|   **ID** Accuracy | 0.936000 | 0.892000 | 0.892000 |

显而易见，**应该选择 1 层**。因此，总的来说，我们可以得到这样一张超参数表格：
##### 最终超参数 GridSearch 搜索结果

| 参数名	|  词向量维度 | 标注向量维度 | batch 大小 | 隐层大小 |  优化算法 | 学习率   |   LSTM 隐层 |
| :----:   | :----:    | :----:      | :----:    | :----:   | :----:   | :----:   | :----:      |
| 参数值    |  64       |   16        |   64      |  128     |   Adam   |   0.1   |   1          |
 
